{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Other Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T06:38:24.448344Z",
     "start_time": "2018-12-26T06:38:22.587891Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "daywisedf = pd.read_csv(\"Week_df.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapterToPool(dataframe, function, supply=[]):\n",
    "\n",
    "    if len(supply) == 0:\n",
    "        supply = getSupply(dataframe)\n",
    "\n",
    "    hold_values = []\n",
    "\n",
    "    try:\n",
    "        pool = Pool(num_pool_to_use)\n",
    "        hold_values = pool.map(function, supply)\n",
    "    finally:\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    return pd.DataFrame(hold_values)\n",
    "\n",
    "\n",
    "def getSupply(dataframe):\n",
    "    supply = []\n",
    "    for x in range(len(dataframe)):\n",
    "        supply.append((x, dataframe.iloc[x]))\n",
    "\n",
    "    return supply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T06:38:26.539860Z",
     "start_time": "2018-12-26T06:38:26.444728Z"
    }
   },
   "outputs": [],
   "source": [
    "def oneHotEncoder(df, col):\n",
    "    lb_style = LabelBinarizer()\n",
    "    lb_results = lb_style.fit_transform(df[col])\n",
    "    lb_new = pd.DataFrame(lb_results, columns=lb_style.classes_)\n",
    "    col_names = lb_new.columns\n",
    "    new_col_names = []\n",
    "    for i in col_names:\n",
    "        new_col_names.append((col+'_'+str(i)))\n",
    "    lb_new.columns = new_col_names\n",
    "    df = pd.concat([df, lb_new], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def addCapFreq(week_df):\n",
    "    show_wise_df = pd.read_csv(\"Daywise_model.csv\")\n",
    "    show_wise_df['dateofshow'] = pd.to_datetime(\n",
    "        show_wise_df['DateTime']).dt.date\n",
    "    lb_style = LabelBinarizer()\n",
    "    lb_results = lb_style.fit_transform(show_wise_df[\"Capacity\"])\n",
    "    lb_Capacity = pd.DataFrame(lb_results, columns=lb_style.classes_)\n",
    "    print(lb_Capacity.columns)\n",
    "    lb_Capacity.columns = ['Cap_110', 'Cap_120', 'Cap_131', 'Cap_310']\n",
    "    show_wise_df = pd.concat([show_wise_df, lb_Capacity], axis=1)\n",
    "    capfreq = show_wise_df.groupby(['dateofshow', 'Movie'], as_index=False).aggregate(\n",
    "        {'Cap_110': 'sum', 'Cap_120': 'sum', 'Cap_131': 'sum', 'Cap_310': 'sum'})  # Same as above\n",
    "    week_df = pd.concat(\n",
    "        [week_df, capfreq[['Cap_110', 'Cap_120', 'Cap_131', 'Cap_310']]], axis=1)\n",
    "    return week_df\n",
    "\n",
    "\n",
    "def addScreenFreq(week_df):\n",
    "    show_wise_df = pd.read_csv(\"Daywise_model.csv\")\n",
    "    show_wise_df['dateofshow'] = pd.to_datetime(\n",
    "        show_wise_df['DateTime']).dt.date\n",
    "    lb_style = LabelBinarizer()\n",
    "    lb_results = lb_style.fit_transform(show_wise_df[\"Screen\"])\n",
    "    lb_Screens = pd.DataFrame(lb_results, columns=lb_style.classes_)\n",
    "    show_wise_df = pd.concat([show_wise_df, lb_Screens], axis=1)\n",
    "    screenfreq = show_wise_df.groupby(['dateofshow', 'Movie'], as_index=False).aggregate(\n",
    "        {'BLUSH': 'sum', 'CARVE': 'sum', 'FRAME': 'sum', 'KITES': 'sum', 'PLUSH': 'sum', 'SPOT': 'sum', 'STREAK': 'sum', 'WEAVE': 'sum'})  # Same as above\n",
    "    week_df = pd.concat([week_df, screenfreq[[\n",
    "        'BLUSH', 'CARVE', 'FRAME', 'KITES', 'PLUSH', 'SPOT', 'STREAK', 'WEAVE']]], axis=1)\n",
    "    return week_df\n",
    "\n",
    "\n",
    "def addTransFreq(week_df):\n",
    "    transdf = pd.read_csv(\"Daywise_model\", index_col=0)\n",
    "    escapeScreens = ['STREAK', 'PLUSH', 'FRAME',\n",
    "                     'SPOT', 'KITES', 'CARVE', 'WEAVE', 'BLUSH']\n",
    "    transdf = transdf[transdf.screen_strname.isin(escapeScreens)]\n",
    "    transdf.dropna(inplace=True)\n",
    "    import datetime as dt\n",
    "    transdf['transt_dtmdatetime'] = pd.to_datetime(\n",
    "        transdf['transt_dtmdatetime'])\n",
    "    transdf['session_dtmshowing'] = pd.to_datetime(\n",
    "        transdf['session_dtmshowing'])\n",
    "    transdf = transdf.groupby(['film_strcode', 'session_dtmshowing', 'transt_dtmdatetime',\n",
    "                               'screen_strname'], as_index=False).aggregate({'transt_intnoofseats': 'sum'})  # Same as above\n",
    "    transdf = transdf.groupby(['film_strcode', 'session_dtmshowing', 'transt_dtmdatetime',\n",
    "                               'screen_strname'], as_index=False).aggregate({'transt_intnoofseats': 'sum'})  # Same as above\n",
    "    transdf = transdf[(transdf[\"transt_intnoofseats\"] >= 1)]\n",
    "    transdf = transdf[(transdf[\"transt_intnoofseats\"] <= 20)]\n",
    "    transdf = oneHotEncoder(transdf, 'transt_intnoofseats')\n",
    "    transdf = transdf.dropna()\n",
    "    transdf['dateofshow'] = pd.to_datetime(\n",
    "        transdf['session_dtmshowing'].dt.date)\n",
    "    transdf.columns = ['Movie', 'session_dtmshowing', 'transt_dtmdatetime', 'screen_strname', 'transt_intnoofseats', 'transt_intnoofseats_1', 'transt_intnoofseats_2', 'transt_intnoofseats_3', 'transt_intnoofseats_4', 'transt_intnoofseats_5', 'transt_intnoofseats_6', 'transt_intnoofseats_7', 'transt_intnoofseats_8',\n",
    "                       'transt_intnoofseats_9', 'transt_intnoofseats_10', 'transt_intnoofseats_11', 'transt_intnoofseats_12', 'transt_intnoofseats_13', 'transt_intnoofseats_14', 'transt_intnoofseats_15', 'transt_intnoofseats_16', 'transt_intnoofseats_17', 'transt_intnoofseats_18', 'transt_intnoofseats_19', 'transt_intnoofseats_20', 'dateofshow']\n",
    "    types_list = ['transt_intnoofseats_1', 'transt_intnoofseats_2', 'transt_intnoofseats_3', 'transt_intnoofseats_4', 'transt_intnoofseats_5', 'transt_intnoofseats_6', 'transt_intnoofseats_7', 'transt_intnoofseats_8', 'transt_intnoofseats_9', 'transt_intnoofseats_10',\n",
    "                  'transt_intnoofseats_11', 'transt_intnoofseats_12', 'transt_intnoofseats_13', 'transt_intnoofseats_14', 'transt_intnoofseats_15', 'transt_intnoofseats_16', 'transt_intnoofseats_17', 'transt_intnoofseats_18', 'transt_intnoofseats_19', 'transt_intnoofseats_20']\n",
    "    types_agg = dict()\n",
    "    for i in types_list:\n",
    "        types_agg[i] = 'sum'\n",
    "    transdf = transdf.groupby(['dateofshow', 'Movie'], as_index=False).aggregate(\n",
    "        types_agg)  # Same as above\n",
    "    transdf.to_csv('DaywiseTransFreq')\n",
    "    del transdf\n",
    "    transdf = pd.read_csv('DaywiseTransFreq', index_col=0)\n",
    "    week_df = pd.merge(week_df, transdf, left_on=[\n",
    "        'Movie', 'dateofshow'], right_on=['Movie', 'dateofshow'], how='left')\n",
    "    week_df = week_df.fillna(0)\n",
    "    return week_df\n",
    "\n",
    "\n",
    "def addSeatsSoldPerTrans(week_df):\n",
    "    showwise = pd.read_csv(\"Daywise_model.csv\", index_col=0)\n",
    "    showwise = showwise.groupby([\"Movie\", \"Date\"], as_index=False).aggregate({\n",
    "        \"SeatsSoldPerTrans\": \"mean\"})\n",
    "    week_df = pd.merge(week_df, showwise, how='left', left_on=[\n",
    "        \"Movie\", \"dateofshow\"], right_on=[\"Movie\", \"Date\"])\n",
    "    return week_df\n",
    "\n",
    "\n",
    "def addLang(week_df):\n",
    "    lang = pd.read_csv(\"Movie_Details.csv\", index_col=0)\n",
    "    lang = lang[['MCode', 'Language']]\n",
    "    week_df = pd.merge(week_df, lang, left_on='Movie',\n",
    "                       right_on='MCode', how='left')\n",
    "    week_df.drop(['MCode'], axis=1, inplace=True)\n",
    "    week_df = week_df.fillna(-1)\n",
    "    return week_df\n",
    "\n",
    "\n",
    "def addRelativeOccupancy(week_wise_df):\n",
    "    tot_occ_week = week_wise_df.groupby(\n",
    "        ['WeekOfShow'], as_index=False).aggregate({'OccAtWeek': 'sum'})\n",
    "    tot_occ_week.rename(columns={'OccAtWeek': 'TotOccAtWeek'}, inplace=True)\n",
    "    week_wise_df = pd.merge(week_df, tot_occ_week, on=[\n",
    "                            'WeekOfShow'], how='left', copy=False)\n",
    "    week_wise_df['RelativeOccAtWeek'] = week_wise_df['OccAtWeek'] / \\\n",
    "        week_wise_df['TotOccAtWeek']\n",
    "    return week_wise_df\n",
    "\n",
    "\n",
    "def getSSPTHistory(supply, num_history_points=7):\n",
    "\n",
    "    index, row = supply\n",
    "    min_date = row['dateofshow'] - timedelta(days=1)\n",
    "    history = norm_df[norm_df['Movie'] == row['Movie']][(\n",
    "        norm_df['dateofshow'] <= min_date)].sort_values(by='dateofshow', ascending=False)\n",
    "\n",
    "    history_pers = history['SeatsSoldPerTrans'].values[:num_history_points]\n",
    "    for i in range(num_history_points):\n",
    "        try:\n",
    "            row[f\"SeatsSoldPerTrans{i}\"] = history_pers[i]\n",
    "        except IndexError:\n",
    "            row[f\"SeatsSoldPerTrans{i}\"] = -1.0\n",
    "\n",
    "    try:\n",
    "        row['SeatsSoldPerTrans_History_Mean'] = statistics.mean(history_pers)\n",
    "        row['SeatsSoldPerTrans_History_Max'] = max(history_pers)\n",
    "        row['SeatsSoldPerTrans_History_Min'] = min(history_pers)\n",
    "    except:\n",
    "        row['SeatsSoldPerTrans_History_Mean'] = -1\n",
    "        row['SeatsSoldPerTrans_History_Max'] = -1\n",
    "        row['SeatsSoldPerTrans_History_Min'] = -1\n",
    "    return row\n",
    "\n",
    "\n",
    "def getRelativeHistory(supply, num_history_points=7):\n",
    "\n",
    "    index, row = supply\n",
    "    min_date = row['WeekOfShow']\n",
    "    history = week_df[week_df['Movie'] == row['Movie']][(\n",
    "        week_df['WeekOfShow'] <= min_date)].sort_values(by='WeekOfShow', ascending=False)\n",
    "\n",
    "    history_pers = history['RelativeOccAtWeek'].values[:num_history_points]\n",
    "    for i in range(num_history_points):\n",
    "        try:\n",
    "            row[f\"RelativeOccAtWeek{i}\"] = history_pers[i]\n",
    "        except IndexError:\n",
    "            row[f\"RelativeOccAtWeek{i}\"] = -1.0\n",
    "\n",
    "    try:\n",
    "        row['RelativeOccAtWeek_History_Mean'] = statistics.mean(history_pers)\n",
    "        row['RelativeOccAtWeek_History_Max'] = max(history_pers)\n",
    "        row['RelativeOccAtWeek_History_Min'] = min(history_pers)\n",
    "    except:\n",
    "        row['RelativeOccAtWeek_History_Mean'] = -1\n",
    "        row['RelativeOccAtWeek_History_Max'] = -1\n",
    "        row['RelativeOccAtWeek_History_Min'] = -1\n",
    "    return row\n",
    "\n",
    "\n",
    "def addHistoryFeatures(week_wise_df):\n",
    "    df = pd.read_csv('Daywise_model.csv', index_col=0)\n",
    "    norm_df = df\n",
    "    supply = getSupply(df[['Movie', 'dateofshow', 'SeatsSoldPerTrans']])\n",
    "    df['dateofshow'] = pd.to_datetime(df['dateofshow']).dt.date\n",
    "    df = adapterToPool(df, getSSPTHistory)\n",
    "    print(\"Added Show History Features\")\n",
    "    df['dateofshow'] = pd.to_datetime(df['dateofshow']).dt.date\n",
    "    week_matrix_daywise['date'] = pd.to_datetime(\n",
    "        week_matrix_daywise['date']).dt.date\n",
    "    df = pd.merge(df, week_matrix_daywise[[\n",
    "                  'date', 'week']], how='left', left_on='dateofshow', right_on='date')\n",
    "    df1 = df.groupby(['Movie', 'week'], as_index=False).aggregate(\n",
    "        {'dateofshow': 'max'})\n",
    "    df1 = pd.merge(df[['Movie', 'dateofshow', 'ShowHistory0', 'ShowHistory1', 'ShowHistory2',\n",
    "                       'ShowHistory3', 'ShowHistory4', 'ShowHistory5', 'ShowHistory6',\n",
    "                       'WalkinHistory0', 'WalkinHistory1', 'WalkinHistory_Mean',\n",
    "                       'WalkinHistory_Max', 'WalkinHistory_Min', 'History_Mean',\n",
    "                       'WalkinHistory2', 'WalkinHistory3', 'WalkinHistory4', 'WalkinHistory5',\n",
    "                       'WalkinHistory6', 'SeatsSoldPerTrans', 'SeatsSoldPerTrans0',\n",
    "                       'SeatsSoldPerTrans1', 'SeatsSoldPerTrans2', 'SeatsSoldPerTrans3',\n",
    "                       'SeatsSoldPerTrans4', 'SeatsSoldPerTrans5', 'SeatsSoldPerTrans6',\n",
    "                       'SeatsSoldPerTrans_History_Mean', 'SeatsSoldPerTrans_History_Max',\n",
    "                       'SeatsSoldPerTrans_History_Min']], df1, how='right', on=['Movie', 'dateofshow'])\n",
    "    week_wise_df = pd.merge(week_wise_df, df1.drop('dateofshow', axis=1), how='left', left_on=[\n",
    "                            'Movie', 'WeekOfShow'], right_on=['Movie', 'week'])\n",
    "    print(\"Added Walkin History Features\")\n",
    "    return week_wise_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-18T11:50:56.979550Z",
     "start_time": "2018-12-18T11:49:38.905135Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2901: DtypeWarning: Columns (3,14,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "/opt/anaconda/lib/python3.6/site-packages/numpy/lib/arraysetops.py:472: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "week_df = addTransFreq(week_df)\n",
    "week_df = addCapFreq(week_df)\n",
    "week_df = addScreenFreq(week_df)\n",
    "week_df = addLang(week_df)\n",
    "week_df = addSeatsSoldPerTrans(week_df)\n",
    "week_df = addRelativeOccupancy(week_df)\n",
    "\n",
    "week_df = addRelativeOccupancy(week_df)\n",
    "week_df = addHistoryFeatures(week_df)\n",
    "week_df = adapterToPool(week_df, getRelativeHistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T09:02:05.967718Z",
     "start_time": "2018-12-15T09:02:03.174154Z"
    }
   },
   "outputs": [],
   "source": [
    "week_df.to_csv('Week_df_features.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
