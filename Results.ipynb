{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T09:20:31.147898Z",
     "start_time": "2019-04-20T09:20:29.406360Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, r2_score, mean_absolute_error, mean_squared_error, confusion_matrix\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from xgboost import XGBRegressor\n",
    "import random\n",
    "from datetime import timedelta\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "from keras.models import model_from_json\n",
    "import warnings\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import pandas as pd\n",
    "from keras.activations import relu\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import keras\n",
    "from tensorflow import set_random_seed\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.random import seed\n",
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "SEED = 69\n",
    "seed(69)\n",
    "set_random_seed(69)\n",
    "np.random.seed(69)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "num_pool_to_use = cpu_count() - 1\n",
    "week_df = pd.read_csv(\"Week_df_features.csv\", index_col=0)\n",
    "week_df['date'] = pd.to_datetime(week_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T09:20:31.347236Z",
     "start_time": "2019-04-20T09:20:31.154331Z"
    }
   },
   "outputs": [],
   "source": [
    "# Considering 2015,16 and 17 data only\n",
    "\n",
    "\n",
    "train_clas = week_df[(week_df['date'].dt.year) != 2017]\n",
    "test_clas = week_df[(week_df['date'].dt.year) == 2017]\n",
    "train = week_df[(week_df['date'].dt.year) != 2017]\n",
    "test = week_df[(week_df['date'].dt.year) == 2017]\n",
    "df_2015 = week_df[(week_df['date'].dt.year) == 2015]\n",
    "df_2016 = week_df[(week_df['date'].dt.year) == 2016]\n",
    "df_2017 = week_df[(week_df['date'].dt.year) == 2017]\n",
    "\n",
    "train_2015, test_2015 = train_test_split(\n",
    "    df_2015, test_size=0.3, shuffle=True, random_state=69)\n",
    "train_2016, test_2016 = train_test_split(\n",
    "    df_2016, test_size=0.3, shuffle=True, random_state=69)\n",
    "train_2017, test_2017 = train_test_split(\n",
    "    df_2017, test_size=0.3, shuffle=True, random_state=69)\n",
    "train_clas = pd.concat([train_2015, train_2016, train_2017])\n",
    "test_clas = pd.concat([test_2015, test_2016, test_2017])\n",
    "train = pd.concat([train_2015, train_2016, train_2017])\n",
    "test = pd.concat([test_2015, test_2016, test_2017])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T09:20:31.564489Z",
     "start_time": "2019-04-20T09:20:31.349073Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WeekOfShow</th>\n",
       "      <th>Movie</th>\n",
       "      <th>ReleaseWeek</th>\n",
       "      <th>LastWeek</th>\n",
       "      <th>Lifetime</th>\n",
       "      <th>WeeksSinceRelease</th>\n",
       "      <th>LastDate</th>\n",
       "      <th>ReleaseDate</th>\n",
       "      <th>OccAtWeek</th>\n",
       "      <th>OccAtPred</th>\n",
       "      <th>...</th>\n",
       "      <th>running_days_in_next_week</th>\n",
       "      <th>Total_MoviesinWeek</th>\n",
       "      <th>Target_Clas_1/7</th>\n",
       "      <th>Target_Clas_2/7</th>\n",
       "      <th>Target_Clas_3/7</th>\n",
       "      <th>Target_Clas_4/7</th>\n",
       "      <th>Target_Clas_5/7</th>\n",
       "      <th>Target_Clas_6/7</th>\n",
       "      <th>Target_Clas_7/7</th>\n",
       "      <th>WeekOfTheYear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>84</td>\n",
       "      <td>HO00001104</td>\n",
       "      <td>84</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-10-29</td>\n",
       "      <td>2015-10-21</td>\n",
       "      <td>1267</td>\n",
       "      <td>563</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>58</td>\n",
       "      <td>HO00000878</td>\n",
       "      <td>56</td>\n",
       "      <td>58</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-04-23</td>\n",
       "      <td>2015-04-10</td>\n",
       "      <td>143</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>85</td>\n",
       "      <td>HO00001134</td>\n",
       "      <td>85</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-11-09</td>\n",
       "      <td>2015-10-31</td>\n",
       "      <td>380</td>\n",
       "      <td>145</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>68</td>\n",
       "      <td>HO00000943</td>\n",
       "      <td>65</td>\n",
       "      <td>69</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2015-07-09</td>\n",
       "      <td>2015-06-12</td>\n",
       "      <td>1968</td>\n",
       "      <td>633</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>55</td>\n",
       "      <td>HO00000814</td>\n",
       "      <td>50</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>2015-02-27</td>\n",
       "      <td>220</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      WeekOfShow       Movie  ReleaseWeek  LastWeek  Lifetime  \\\n",
       "960           84  HO00001104           84        85         1   \n",
       "583           58  HO00000878           56        58         2   \n",
       "1005          85  HO00001134           85        86         1   \n",
       "823           68  HO00000943           65        69         4   \n",
       "507           55  HO00000814           50        55         5   \n",
       "\n",
       "      WeeksSinceRelease    LastDate ReleaseDate  OccAtWeek  OccAtPred  \\\n",
       "960                   0  2015-10-29  2015-10-21       1267        563   \n",
       "583                   2  2015-04-23  2015-04-10        143         22   \n",
       "1005                  0  2015-11-09  2015-10-31        380        145   \n",
       "823                   3  2015-07-09  2015-06-12       1968        633   \n",
       "507                   5  2015-04-01  2015-02-27        220         32   \n",
       "\n",
       "          ...        running_days_in_next_week  Total_MoviesinWeek  \\\n",
       "960       ...                                2                  26   \n",
       "583       ...                                0                  21   \n",
       "1005      ...                                6                  37   \n",
       "823       ...                                2                  31   \n",
       "507       ...                                0                  27   \n",
       "\n",
       "      Target_Clas_1/7  Target_Clas_2/7  Target_Clas_3/7  Target_Clas_4/7  \\\n",
       "960               1.0              1.0              0.0              0.0   \n",
       "583               0.0              0.0              0.0              0.0   \n",
       "1005              1.0              1.0              1.0              1.0   \n",
       "823               1.0              1.0              0.0              0.0   \n",
       "507               0.0              0.0              0.0              0.0   \n",
       "\n",
       "      Target_Clas_5/7  Target_Clas_6/7  Target_Clas_7/7  WeekOfTheYear  \n",
       "960               0.0              0.0              0.0             44  \n",
       "583               0.0              0.0              0.0             18  \n",
       "1005              1.0              1.0              0.0             45  \n",
       "823               0.0              0.0              0.0             28  \n",
       "507               0.0              0.0              0.0             15  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T09:20:31.662101Z",
     "start_time": "2019-04-20T09:20:31.566560Z"
    }
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "    'WeeksSinceRelease',\n",
    "    'Capacity', 'Cap_110', 'Cap_120', 'Cap_131', 'Cap_310',\n",
    "    'ShowsInWeek', 'BLUSH', 'CARVE', 'FRAME', 'KITES', 'PLUSH', 'SPOT', 'STREAK',\n",
    "    'WEAVE', 'trans_1', 'trans_2', 'trans_3', 'trans_4', 'trans_5',\n",
    "    'trans_6', 'trans_7', 'trans_8', 'trans_9', 'trans_10', 'trans_11',\n",
    "    'trans_12', 'OccCumSum', 'CapCumSum', 'OccCumSumPer',\n",
    "    'OtherReleasesInWeek', 'Walkin', 'WalkinPer', 'OccPer',\n",
    "    'Month',\n",
    "    'ShowHistory0', 'ShowHistory1', 'ShowHistory2',\n",
    "    'ShowHistory3', 'ShowHistory4', 'ShowHistory5', 'ShowHistory6',\n",
    "    'WalkinHistory0', 'WalkinHistory1', 'WalkinHistory_Mean',\n",
    "    'WalkinHistory_Max', 'WalkinHistory_Min', 'History_Mean',\n",
    "    'WalkinHistory2', 'WalkinHistory3', 'WalkinHistory4', 'WalkinHistory5',\n",
    "    'WalkinHistory6', 'SeatsSoldPerTrans0',\n",
    "    'SeatsSoldPerTrans1', 'SeatsSoldPerTrans2', 'SeatsSoldPerTrans3',\n",
    "    'SeatsSoldPerTrans4', 'SeatsSoldPerTrans5', 'SeatsSoldPerTrans6',\n",
    "    'SeatsSoldPerTrans_History_Mean', 'SeatsSoldPerTrans_History_Max',\n",
    "    'SeatsSoldPerTrans_History_Min', 'RelativeOccAtWeek0',\n",
    "    'RelativeOccAtWeek1', 'RelativeOccAtWeek2', 'RelativeOccAtWeek3',\n",
    "    'RelativeOccAtWeek4', 'RelativeOccAtWeek5', 'RelativeOccAtWeek6',\n",
    "    'RelativeOccAtWeek_History_Mean', 'RelativeOccAtWeek_History_Max',\n",
    "    'RelativeOccAtWeek_History_Min', 'Total_MoviesinWeek', 'Drama', 'Comedy', 'Romance', 'Runtime', 'Language']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Two Stage-ET+DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T09:20:32.796605Z",
     "start_time": "2019-04-20T09:20:32.792468Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    json_file = open('/mnt/home/spicrowdpred/Spi-Occ/Lifetime/model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\n",
    "        \"/mnt/home/spicrowdpred/Spi-Occ/Lifetime/model.h5\")\n",
    "    print(f\"Loaded model from disk\")\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-20T09:22:57.791002Z",
     "start_time": "2019-04-20T09:22:54.820729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 is length of importantfeatures\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.92      0.94       312\n",
      "         1.0       0.97      0.99      0.98       743\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      1055\n",
      "   macro avg       0.97      0.95      0.96      1055\n",
      "weighted avg       0.97      0.97      0.97      1055\n",
      "\n",
      "[[288  24]\n",
      " [ 10 733]]\n",
      "Loaded model from disk\n",
      "R SQURED SCORE      : 0.3073924663550127\n",
      "MAE                 : 1.1610977039413048\n",
      "MSE                 : 4.67174451076025\n",
      "RMSE                : 2.16142187246272\n",
      "0 Week Error:     64.854111\n",
      "1 Week Error:     19.098143\n",
      "2 Week Error:      7.824934\n",
      ">3 Week Error:     4.774536\n",
      "3 Week Error:      3.448276\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "usefulFeatures = ['Capacity', 'DaysShowedInWeek', 'Total_MoviesinWeek',\n",
    "                  'WeekOfTheYear', 'ShowsInWeek', 'ShowHistory1', 'RelativeOccAtWeek0',\n",
    "                  'trans_2', 'trans_3', 'OccPer', 'RelativeOccAtWeek_History_Min']\n",
    "print(len(usefulFeatures), \"is length of importantfeatures\")\n",
    "etc = ExtraTreesClassifier(n_estimators=200, min_samples_split=3,\n",
    "                           min_samples_leaf=1, max_depth=None, bootstrap=False, n_jobs=-1)\n",
    "etc.fit(train_clas[usefulFeatures], train_clas['Target_Clas'])\n",
    "\n",
    "predictions_clas = (etc.predict(test_clas[usefulFeatures]))\n",
    "\n",
    "print(classification_report(test_clas[\"Target_Clas\"], predictions_clas))\n",
    "print(confusion_matrix(test_clas[\"Target_Clas\"], predictions_clas))\n",
    "classification_confusion_matrix = classification_report(\n",
    "    test_clas[\"Target_Clas\"], predictions_clas)\n",
    "test_clas['predicted'] = predictions_clas\n",
    "test['Clas_Pred'] = predictions_clas\n",
    "test_reg = test_clas[test_clas['predicted'] == 1]\n",
    "train_reg = train[train['Target'] > 0]\n",
    "\n",
    "seed(69)\n",
    "set_random_seed(69)\n",
    "np.random.seed(69)\n",
    "\n",
    "test_clas['predicted'] = predictions_clas\n",
    "test['Clas_Pred'] = predictions_clas\n",
    "test_reg = test_clas[test_clas['predicted'] == 1]\n",
    "\n",
    "test_reg = test_reg[test_reg['week_x'] != 192]\n",
    "\n",
    "train_reg = train[train['Target'] > 0]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(train_reg[features].shape[-1],)))\n",
    "model.add(keras.layers.LeakyReLU())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(64))\n",
    "model.add(keras.layers.LeakyReLU())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(32))\n",
    "model.add(keras.layers.LeakyReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(8))\n",
    "model.add(keras.layers.LeakyReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1,))\n",
    "opzr = RMSprop(lr=3e-3)\n",
    "model.compile(optimizer=opzr, loss='mae')  # mae has 60% no error\n",
    "\n",
    "\n",
    "scalerX = StandardScaler().fit(train_reg[features])  # Standard Scalar\n",
    "scalery = StandardScaler().fit(np.array(train_reg['Target']).reshape(-1, 1))\n",
    "X_train = scalerX.transform(train_reg[features])\n",
    "y_train = scalery.transform(np.array(train_reg['Target']).reshape(-1, 1))\n",
    "X_test = scalerX.transform(test_reg[features])\n",
    "y_test = scalery.transform(np.array(test_reg['Target']).reshape(-1, 1))\n",
    "seed(69)\n",
    "set_random_seed(69)\n",
    "np.random.seed(69)\n",
    "\n",
    "# model.fit(X_train,y_train,epochs=14,batch_size=32,verbose=1)\n",
    "model = load_model()\n",
    "ann_pred = (model.predict(scalerX.transform(test_reg[features].values)))\n",
    "ann_pred = scalery.inverse_transform(ann_pred)\n",
    "print(\"R SQURED SCORE      :\", r2_score(test_reg[\"Target\"], ann_pred))\n",
    "print(\"MAE                 :\", mean_absolute_error(\n",
    "    test_reg[\"Target\"].values, ann_pred))\n",
    "print(\"MSE                 :\", mean_squared_error(\n",
    "    test_reg[\"Target\"].values, ann_pred))\n",
    "print(\"RMSE                :\", np.sqrt(\n",
    "    mean_squared_error(test_reg[\"Target\"].values, ann_pred)))\n",
    "ann_error_df = pd.DataFrame()\n",
    "ann_error_df = test_reg[[\"Movie\", \"Target\",\n",
    "                         \"Capacity\", 'Lifetime', 'Language']]\n",
    "ann_error_df['Pred'] = ann_pred\n",
    "ann_error_df[\"error\"] = (abs(ann_error_df[\"Target\"]-ann_error_df[\"Pred\"]))\n",
    "ann_error_df.loc[ann_error_df[\"error\"] < 1, \"count\"] = '0 Week Error:'\n",
    "ann_error_df.loc[((ann_error_df[\"error\"] >= 1) & (\n",
    "    ann_error_df[\"error\"] < 2)), \"count\"] = '1 Week Error:'\n",
    "ann_error_df.loc[((ann_error_df[\"error\"] >= 2) & (\n",
    "    ann_error_df[\"error\"] < 3)), \"count\"] = '2 Week Error:'\n",
    "ann_error_df.loc[((ann_error_df[\"error\"] >= 3) & (\n",
    "    ann_error_df[\"error\"] < 4)), \"count\"] = '3 Week Error:'\n",
    "ann_error_df.loc[ann_error_df[\"error\"] >= 4, \"count\"] = '>3 Week Error:'\n",
    "ann_error = ann_error_df[\"count\"].value_counts()\n",
    "ann_error = (ann_error*100)/sum(ann_error)\n",
    "print(ann_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THRESHOLD (DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T08:30:30.984225Z",
     "start_time": "2019-04-14T08:30:30.880697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall for 0 as 1 /7\n",
      "0.7788461538461539\n",
      "312\n",
      "[[243  69]\n",
      " [  8 735]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.78      0.86       312\n",
      "         1.0       0.91      0.99      0.95       743\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1055\n",
      "   macro avg       0.94      0.88      0.91      1055\n",
      "weighted avg       0.93      0.93      0.92      1055\n",
      "\n",
      "------------------------------\n",
      "recall for 0 as 2 /7\n",
      "0.8525641025641025\n",
      "312\n",
      "[[266  46]\n",
      " [ 12 731]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.85      0.90       312\n",
      "         1.0       0.94      0.98      0.96       743\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      1055\n",
      "   macro avg       0.95      0.92      0.93      1055\n",
      "weighted avg       0.95      0.95      0.94      1055\n",
      "\n",
      "------------------------------\n",
      "recall for 0 as 3 /7\n",
      "0.875\n",
      "312\n",
      "[[273  39]\n",
      " [ 15 728]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.88      0.91       312\n",
      "         1.0       0.95      0.98      0.96       743\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      1055\n",
      "   macro avg       0.95      0.93      0.94      1055\n",
      "weighted avg       0.95      0.95      0.95      1055\n",
      "\n",
      "------------------------------\n",
      "recall for 0 as 4 /7\n",
      "0.8814102564102564\n",
      "312\n",
      "[[275  37]\n",
      " [ 20 723]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.88      0.91       312\n",
      "         1.0       0.95      0.97      0.96       743\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      1055\n",
      "   macro avg       0.94      0.93      0.93      1055\n",
      "weighted avg       0.95      0.95      0.95      1055\n",
      "\n",
      "------------------------------\n",
      "recall for 0 as 5 /7\n",
      "0.8910256410256411\n",
      "312\n",
      "[[278  34]\n",
      " [ 29 714]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.89      0.90       312\n",
      "         1.0       0.95      0.96      0.96       743\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      1055\n",
      "   macro avg       0.93      0.93      0.93      1055\n",
      "weighted avg       0.94      0.94      0.94      1055\n",
      "\n",
      "------------------------------\n",
      "recall for 0 as 6 /7\n",
      "0.9006410256410257\n",
      "312\n",
      "[[281  31]\n",
      " [ 38 705]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.90      0.89       312\n",
      "         1.0       0.96      0.95      0.95       743\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1055\n",
      "   macro avg       0.92      0.92      0.92      1055\n",
      "weighted avg       0.94      0.93      0.93      1055\n",
      "\n",
      "------------------------------\n",
      "recall for 0 as 7 /7\n",
      "0.9262820512820513\n",
      "312\n",
      "[[289  23]\n",
      " [ 86 657]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.93      0.84       312\n",
      "         1.0       0.97      0.88      0.92       743\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      1055\n",
      "   macro avg       0.87      0.91      0.88      1055\n",
      "weighted avg       0.91      0.90      0.90      1055\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "test['predicted'] = predictions\n",
    "\n",
    "test.loc[test[\"predicted\"] >= (1/7), \"predicted_Clas_1\"] = 1\n",
    "test.loc[test[\"predicted\"] < (1/7), \"predicted_Clas_1\"] = 0\n",
    "\n",
    "test.loc[test[\"predicted\"] >= (2/7), \"predicted_Clas_2\"] = 1\n",
    "test.loc[test[\"predicted\"] < (2/7), \"predicted_Clas_2\"] = 0\n",
    "\n",
    "test.loc[test[\"predicted\"] >= (3/7), \"predicted_Clas_3\"] = 1\n",
    "test.loc[test[\"predicted\"] < (3/7), \"predicted_Clas_3\"] = 0\n",
    "\n",
    "\n",
    "test.loc[test[\"predicted\"] >= (4/7), \"predicted_Clas_4\"] = 1\n",
    "test.loc[test[\"predicted\"] < (4/7), \"predicted_Clas_4\"] = 0\n",
    "\n",
    "test.loc[test[\"predicted\"] >= (5/7), \"predicted_Clas_5\"] = 1\n",
    "test.loc[test[\"predicted\"] < (5/7), \"predicted_Clas_5\"] = 0\n",
    "\n",
    "test.loc[test[\"predicted\"] >= (6/7), \"predicted_Clas_6\"] = 1\n",
    "test.loc[test[\"predicted\"] < (6/7), \"predicted_Clas_6\"] = 0\n",
    "\n",
    "test.loc[test[\"predicted\"] >= (7/7), \"predicted_Clas_7\"] = 1\n",
    "test.loc[test[\"predicted\"] < (7/7), \"predicted_Clas_7\"] = 0\n",
    "\n",
    "\n",
    "test.loc[test[\"Target\"] > 0, \"Target_Class\"] = 1\n",
    "test.loc[test[\"Target\"] <= 0, \"Target_Class\"] = 0\n",
    "\n",
    "confusion_matrices = []\n",
    "for i in range(1, 8):\n",
    "    confusion_matrices.append(confusion_matrix(\n",
    "        test[\"Target_Class\"], test[\"predicted_Clas_\"+str(i)]))\n",
    "    # print(confusion_matrices[i-1])\n",
    "    # print(confusion_matrices[i-1][0][0])\n",
    "    print(\"recall for 0 as\", i, \"/7\")\n",
    "    print(confusion_matrices[i-1][0][0]/sum(confusion_matrices[i-1][0]))\n",
    "    print(sum(confusion_matrices[i-1][0]))\n",
    "#     print(\"*****-------------------*************\")\n",
    "#     print(\"classification results---------------------------------\")\n",
    "#     print(classification_confusion_matrix)\n",
    "#     print(\"-----------------end------------------------------------\")\n",
    "    print(confusion_matrices[i-1])\n",
    "    print(classification_report(\n",
    "        test[\"Target_Class\"], test[\"predicted_Clas_\"+str(i)]))\n",
    "    # print(confusion_matrices(test[\"Target_Class\"],test[\"predicted_Clas_\"+str(i)]))\n",
    "    print\n",
    "    print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly Thresholds were obtained for XGB and ET"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
